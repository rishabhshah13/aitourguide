{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key = os.getenv('OPENAI_API_KEY2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://haystack.deepset.ai/integrations/ollama\n",
    "\n",
    "# from haystack.dataclasses import ChatMessage\n",
    "\n",
    "# from haystack_integrations.components.generators.ollama import OllamaChatGenerator\n",
    "\n",
    "# messages = [\n",
    "#     ChatMessage.from_user(\"What's Natural Language Processing?\"),\n",
    "#     ChatMessage.from_system(\n",
    "#         \"Natural Language Processing (NLP) is a field of computer science and artificial \"\n",
    "#         \"intelligence concerned with the interaction between computers and human language\"\n",
    "#     ),\n",
    "#     ChatMessage.from_user(\"How do I get started?\"),\n",
    "# ]\n",
    "# client = OllamaChatGenerator(model=\"gemma2b\", timeout=45, url=\"http://127.0.0.1:11434/api/chat/\")\n",
    "\n",
    "# response = client.run(messages, generation_kwargs={\"temperature\": 0.2})\n",
    "\n",
    "# print(response[\"replies\"][0].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"gemma2:2b-text-q5_K_S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='gemma2:2b-text-q5_K_S', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import pipeline\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"google/gemma-2-2b\",\n",
    "#     device=\"mps\",  # replace with \"mps\" to run on a Mac device\n",
    "#     token=os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "# )\n",
    "\n",
    "# text = \"Once upon a time,\"\n",
    "# outputs = pipe(text, max_new_tokens=256)\n",
    "# response = outputs[0][\"generated_text\"]\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/abetlen/llama-cpp-python/tree/main\n",
    "# https://agi-sphere.com/install-llama-mac/\n",
    "# https://ollama.com/library/gemma2:2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tourguideenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
